{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LBqj0gIiUcEF"
      },
      "source": [
        "# Домашнее задание 3. Детекция объектов\n",
        "\n",
        "Сыграем в квиддич? Или лучше в карты?\n",
        "\n",
        "В этом дз вам предстоит написать практически с нуля архитектуру для детекции, а также воспользоваться готовым решением. На выбор даётся два датасета, отличаются они только картинками. Форматы, баллы - все одинаково.\n",
        "\n",
        "Первый вариант это датасет по кадрам игры в квиддич из Гарри Поттера. Если вы забыли правила, то нажмите [сюда](https://harrypotter.fandom.com/ru/wiki/%D0%9A%D0%B2%D0%B8%D0%B4%D0%B4%D0%B8%D1%87). Вы научитесь искать и выделять на фотографиях бладжеры, квоффл и снитч.\n",
        "\n",
        "Второй вариант это датасет с игральными картами. Если вы забыли что такое карты, то нажмите [сюда](https://ru.wikipedia.org/wiki/%D0%98%D0%B3%D1%80%D0%B0%D0%BB%D1%8C%D0%BD%D1%8B%D0%B5_%D0%BA%D0%B0%D1%80%D1%82%D1%8B). Вы научитесь искать и выделять на фотографиях несколько типов карт.\n",
        "\n",
        "Оба варианта содержат около 300 картинок, данные хранятся в xml в формате PascalVOC. Есть малые отличия, но ничего страшного.\n",
        "\n",
        "\n",
        "Если с самописным детектором совсем не получается, то можно после создания датасетов перейти к концу, где обучается готовый, с ним будет проще :)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6Mkra5AA0rV"
      },
      "source": [
        "# Notes\n",
        "\n",
        "Дз проверялось на работоспособность в colab. Не гарантируется, что будет работать на чем-то другом, и точно не будет работать из коробки на Windows.\n",
        "\n",
        "По вопросам формулировок (не ошибок торча!), в случае отсутствия ответа в общем чате (поиск по чату позволяет проверить), можно написать в него с тегом @markblumenau."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8bl6rVf-F_1"
      },
      "source": [
        "## Данные\n",
        "\n",
        "Скачайте один из датасетов на свой вкус и начните работу с ним.\n",
        "Разметка находится в xmls папке, картинки в images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f2ByC-_g-Kdn",
        "outputId": "edbc041d-672f-42e5-d04d-394d02c8b5a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-12-08 09:58:45--  https://github.com/markblumenau/hw3_iad_dl/raw/main/harry/data.zip\n",
            "Resolving github.com (github.com)... 192.30.255.112\n",
            "Connecting to github.com (github.com)|192.30.255.112|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/markblumenau/hw3_iad_dl/main/harry/data.zip [following]\n",
            "--2023-12-08 09:58:45--  https://raw.githubusercontent.com/markblumenau/hw3_iad_dl/main/harry/data.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 17592727 (17M) [application/zip]\n",
            "Saving to: ‘data.zip’\n",
            "\n",
            "data.zip            100%[===================>]  16.78M  --.-KB/s    in 0.08s   \n",
            "\n",
            "2023-12-08 09:58:46 (214 MB/s) - ‘data.zip’ saved [17592727/17592727]\n",
            "\n",
            "--2023-12-08 09:58:46--  https://github.com/markblumenau/hw3_iad_dl/raw/main/cards/data.zip\n",
            "Resolving github.com (github.com)... 192.30.255.113\n",
            "Connecting to github.com (github.com)|192.30.255.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/markblumenau/hw3_iad_dl/main/cards/data.zip [following]\n",
            "--2023-12-08 09:58:46--  https://raw.githubusercontent.com/markblumenau/hw3_iad_dl/main/cards/data.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 38392108 (37M) [application/zip]\n",
            "Saving to: ‘data.zip.1’\n",
            "\n",
            "data.zip.1          100%[===================>]  36.61M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2023-12-08 09:58:47 (262 MB/s) - ‘data.zip.1’ saved [38392108/38392108]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Harry Potter -- uncomment\n",
        "!wget https://github.com/markblumenau/hw3_iad_dl/raw/main/harry/data.zip\n",
        "\n",
        "# Cards -- uncomment\n",
        "!wget https://github.com/markblumenau/hw3_iad_dl/raw/main/cards/data.zip\n",
        "\n",
        "\n",
        "\n",
        "!unzip -q data.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CkroFQHZXZwQ"
      },
      "source": [
        "# Задача 1. 0.5 балла.\n",
        "\n",
        "Ниже написан код для стандартного Dataset из библиотеки pytorch. Dataset требует реализации `__getitem__` и `__len__` методов. Далее эти методы будут использованы для формирования батчей для обучения. Поскольку читать придется из xml файлов, нужно перед этим дописать функцию get_xml_data, чтобы по названию картинки подтягивать аннотации."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "j-v0_fX4A0rW"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from xml.etree import ElementTree as ET\n",
        "import albumentations as A\n",
        "from albumentations.pytorch.transforms import ToTensorV2\n",
        "from pathlib import Path\n",
        "import glob\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torchvision\n",
        "from torchvision.models import ResNet50_Weights\n",
        "from tqdm.notebook import tqdm\n",
        "from torch import nn\n",
        "import shutil\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.patches import Rectangle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-7_gArrwBnq"
      },
      "source": [
        "Функции можно и нужно передать некий class_dict. Он есть и при инициализации датасета ниже. С его помощью можно название класса превратить в int. Далее подразумевается, что класс идёт как int."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 210,
      "metadata": {
        "id": "8wwipnnkA0rX"
      },
      "outputs": [],
      "source": [
        "def get_xml_data(image_name, root, class_dict, xml_prefix=\"/xmls/\"):\n",
        "    # get smth like ZZZ/YYY/XXXXXX.jpg -> XXXXXX\n",
        "    filename = image_name.split(\"/\")[-1].split(\".\")[0]\n",
        "    # read xml\n",
        "    print(str(root) + xml_prefix + filename + \".xml\")\n",
        "    tree = ET.parse(str(root) + xml_prefix + filename + \".xml\")\n",
        "    treeroot = tree.getroot()\n",
        "    # iterate over bboxes\n",
        "    bboxes = []\n",
        "    print(treeroot.find(\"size\").keys())\n",
        "    for tag in treeroot.findall(\"object\"):\n",
        "        # you need: xmin ymin xmax ymax, class in dict\n",
        "        s = tag.find('bndbox').find('xmin').text\n",
        "\n",
        "        print(s)\n",
        "        # name\n",
        "# pose\n",
        "# truncated\n",
        "# Difficult\n",
        "# bndbox\n",
        "\n",
        "        # bboxes.append(res)\n",
        "\n",
        "    return bboxes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 211,
      "metadata": {
        "id": "FqnUStDzgGnS"
      },
      "outputs": [],
      "source": [
        "class PascalDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, *, transform, root=\"dataset\", train=True, seed=42):\n",
        "        self.root = Path(root)\n",
        "        self.transform = transform\n",
        "\n",
        "        assert self.root.is_dir(), f\"No data at `{root}`\"\n",
        "\n",
        "        self.filenames = np.array(glob.glob(root + \"/images/*\"))\n",
        "        with open(str(self.root) + \"/class_dict\", \"r\") as f:\n",
        "            self.class_dict = eval(f.readline())\n",
        "\n",
        "        self.class_dict_inv = {v: k for k, v in self.class_dict.items()}\n",
        "\n",
        "        np.random.seed(seed)\n",
        "        permutation = np.random.permutation(len(self.filenames))\n",
        "\n",
        "        # Train/test split\n",
        "        if train:\n",
        "            self.filenames = self.filenames[\n",
        "                permutation[: int(len(self.filenames) * 0.9)]\n",
        "            ].tolist()\n",
        "        else:\n",
        "            self.filenames = self.filenames[\n",
        "                permutation[int(len(self.filenames) * 0.9) :]\n",
        "            ].tolist()\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        ### YOUR CODE HERE ###\n",
        "\n",
        "        fname = self.filenames[idx]\n",
        "        image = [] # change!\n",
        "        bboxes = [] # change!\n",
        "\n",
        "        return self.transform(image=image, bboxes=bboxes)\n",
        "\n",
        "    def __get_raw_item__(self, idx):\n",
        "        fname = self.filenames[idx]\n",
        "        print(fname)\n",
        "        return fname, get_xml_data(fname, self.root, self.class_dict)\n",
        "\n",
        "    def __len__(self):\n",
        "        ### YOUR CODE HERE ###\n",
        "\n",
        "        return 0 # change!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upuJ9g4SXwtx"
      },
      "source": [
        "Ниже определяем стандартные нормализации и приведение размера к 512x512.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 212,
      "metadata": {
        "id": "Xj5DYhiknhAp"
      },
      "outputs": [],
      "source": [
        "mean = (0.485, 0.456, 0.406)\n",
        "std = (0.229, 0.224, 0.225)\n",
        "\n",
        "train_transform = A.Compose(\n",
        "    [\n",
        "        A.Resize(512, 512),\n",
        "        A.augmentations.transforms.Normalize(mean=mean, std=std),\n",
        "        ToTensorV2(),\n",
        "    ],\n",
        "    bbox_params=dict(format=\"pascal_voc\", min_visibility=0.3),\n",
        ")\n",
        "\n",
        "test_transform = A.Compose(\n",
        "    [\n",
        "        A.Resize(512, 512),\n",
        "        A.augmentations.transforms.Normalize(mean=mean, std=std),\n",
        "        ToTensorV2(),\n",
        "    ],\n",
        "    bbox_params=dict(format=\"pascal_voc\", min_visibility=0.5),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 213,
      "metadata": {
        "id": "mHhV_8UXEsIB",
        "outputId": "0cacb6ea-3567-4b08-b145-52f737270204",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "./data//images/238.jpg\n",
            "data/xmls/238.xml\n",
            "[]\n",
            "269\n",
            "428\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('./data//images/238.jpg', [])"
            ]
          },
          "metadata": {},
          "execution_count": 213
        }
      ],
      "source": [
        "train_ds = PascalDataset(root=\"./data/\", transform=train_transform, train=True)\n",
        "test_ds = PascalDataset(root=\"./data/\", transform=test_transform, train=False)\n",
        "train_ds.__get_raw_item__(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YhWaKC0NTYPL"
      },
      "source": [
        "# Задача 2. 1 балл.\n",
        "\n",
        "Теперь, когда мы загрузили данные, хорошо бы посмотреть на них, прежде чем обучать какие-либо модели. Напишите функцию `visualize`, которая принимает списки изображений и прямоугольников в качестве входных данных и рисует эти прямоугольники на изображениях.\n",
        "\n",
        "В датасете есть class_dict_inv, который позволит вам сделать обратное преобразование: int, содержащий класс, в строку с названием.\n",
        "\n",
        "\n",
        "Полезные функции:\n",
        "* [plt.subplots](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.subplots.html) -- легко создавать несколько изображений в одной pyplot figure\n",
        "* [ax.imshow](https://matplotlib.org/stable/api/_as_gen/matplotlib.axes.Axes.imshow.html) -- отображение графиков (не забудьте откатить нормализацию)\n",
        "* [ax.text](https://matplotlib.org/stable/api/_as_gen/matplotlib.axes.Axes.text.html), [patches.Rectangle](https://matplotlib.org/stable/api/_as_gen/matplotlib.patches.Rectangle.html) -- для рисования прямоугольников и текста с аннотацией"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qMlang0KJHS7"
      },
      "outputs": [],
      "source": [
        "def visualize(images, bboxes):\n",
        "    mean = (0.485, 0.456, 0.406)\n",
        "    std = (0.229, 0.224, 0.225)\n",
        "\n",
        "    fig, axes = plt.subplots(\n",
        "        2, len(images) // 2 + len(images) % 2, figsize=(10, 8), dpi=100\n",
        "    )\n",
        "\n",
        "    for i, ax in enumerate(axes.reshape(-1)):\n",
        "\n",
        "        ax.axis(False)\n",
        "\n",
        "        if i >= len(images):\n",
        "            break\n",
        "        # Вот тут нужно выполнить permute (вспомните где у torch каналы, а где они у matplotlib)\n",
        "        # И откатить нормализацию (просто обратное преобразование)\n",
        "        # Имена и количества классов можно подтянуть из датасета через train_ds.class_dict_inv\n",
        "\n",
        "        ax.imshow(### YOUR CODE HERE ###)\n",
        "\n",
        "        for bbox in bboxes[i]:\n",
        "        ### YOUR CODE HERE ###\n",
        "\n",
        "    fig.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6oHM4iPKTPzk"
      },
      "source": [
        "У вас должно получиться что-то похожее на изображения для датасета с масками:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8WSPRNrKA0rY"
      },
      "source": [
        "![image](https://gcdnb.pbrd.co/images/n8BnhhfCgYnQ.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zuFWYK8sA0rZ"
      },
      "outputs": [],
      "source": [
        "out = [train_ds[i] for i in range(6)]\n",
        "visualize([o[\"image\"] for o in out], [o[\"bboxes\"] for o in out])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ReVJR_kYVTfO"
      },
      "source": [
        "# Задача 3. 3 балла.\n",
        "## YOLO-like детектор\n",
        "\n",
        "Сейчас нам предстоить реализовать детектор, похожий на YOLO. Это один из самых простых детекторов с точки зрения реализации. YOLO описан в статье: [You Only Look Once: Unified, Real-Time Object Detection](https://arxiv.org/abs/1506.02640). Здесь мы его немного изменим и упростим. Будем использовать ResNet для извлечения признаков. На выходе мы будем получать карту признаков размера 16x16."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SgE_sv_jPCet"
      },
      "source": [
        "We convert lists of bounding boxes to the target downsampled grid. For this we\n",
        "\n",
        "* compute centers of bounding boxes ($c_x, c_y$)\n",
        "* change center coordinates (offset from the top left corner for each corresponding grid cell on a small grid)\n",
        "* normalize box height and with to $[0, 1]$\n",
        "* fill the target grid with values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3738iQSA0ra"
      },
      "source": [
        "## Задача 3.1. 1 балл.\n",
        "\n",
        "Первым делом нам нужно реализовать collate function. Это функция позволит нам кастомизировать, как именно батч конструируется из примеров (смотрите [pytorch docs](https://pytorch.org/docs/stable/data.html#dataloader-collate-fn) для деталей).\n",
        "\n",
        "Это функция должна принять на вход лист прямоугольников и сгенерировать тензор размера Bx16x16x6. Первая размерность - это количество примеров в батче. Далее идут две пространственные размерности, это сетка 16 на 16.\n",
        "\n",
        "В каналах у нас будут записаны:\n",
        "* Сдвиги центра bbox относительно начала клеточки (клеточка это \"пиксель\" на изображении 16 на 16 на выходе сети). Записаны эти сдвиги будут в клеточку, к которой относятся. 2 канала (X, Y)\n",
        "* Нормализованные ширина и высота bbox. 2 канала (W, H)\n",
        "* Confidence сетки. Им мы будем пользоваться, чтобы фильтровать уверенность сетки в наличии bbox в данной клетке. Таргет содержит 1 там, где bbox есть, и 0 иначе. 1 канал\n",
        "* Класс детекции (тот самый int, полученный из строки с названием)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NzOTozOVKzt1"
      },
      "outputs": [],
      "source": [
        "def collate_fn(batch, downsample=32):\n",
        "    imgs, batch_boxes = map(list, (zip(*[(b[\"image\"], b[\"bboxes\"]) for b in batch])))\n",
        "\n",
        "    imgs = torch.stack(imgs)\n",
        "    b, _, h, w = imgs.shape\n",
        "\n",
        "    target = imgs.new_zeros(b, 6, h // downsample, w // downsample)\n",
        "\n",
        "    # Add sample index to targets\n",
        "    for i, boxes in enumerate(batch_boxes):\n",
        "        xmin, ymin, xmax, ymax, classes = map(\n",
        "            torch.squeeze, torch.split(imgs.new_tensor(boxes), 1, dim=-1)\n",
        "        )\n",
        "\n",
        "        # Нормализуйте ширину и высоту, поделив на ширину и высоту исходного изображения\n",
        "        x_cell =  # TODO размер клетки по X в пикс\n",
        "        y_cell =  # TODO размер клетки по Y в пикс\n",
        "        w_box =  # TODO ширина бокса отнормированная\n",
        "        h_box =  # TODO высота бокса отнормированная\n",
        "\n",
        "        # Посчитайте координаты центра и сдвиги\n",
        "        cx =  # TODO (координаты центра в исходных координатах)\n",
        "        cy =  # TODO\n",
        "        cx_idx =  # TODO (посчитайте индекс центра на карте признаков размера 16x16. Это будут как раз координаты пикселя, куда мы запишем параметры коробки)\n",
        "        cy_idx =  # TODO\n",
        "\n",
        "        cx_box =  # TODO (посчитайте сдивиги относительно cx_idx)\n",
        "        cy_box =  # TODO\n",
        "\n",
        "        target[i, :, cy_idx, cx_idx] = torch.stack(\n",
        "            [cx_box, cy_box, w_box, h_box, torch.ones_like(cx_box), classes]\n",
        "        )\n",
        "\n",
        "    return {\"image\": imgs, \"target\": target}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sqpwb_7lU3_u"
      },
      "source": [
        "Ниже вы можете увидеть пример, как выглядит решетка размера 16 на 16 на исходном изображении:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZWXgBxyrU-mX"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(5, 5))\n",
        "i = 20\n",
        "\n",
        "img = train_ds[i][\"image\"].permute(1, 2, 0) * torch.tensor(std).view(\n",
        "    1, 1, -1\n",
        ") + torch.tensor(mean).view(1, 1, -1)\n",
        "bboxes = torch.tensor(train_ds[i][\"bboxes\"])\n",
        "\n",
        "ax.imshow(img)\n",
        "loc = plt.matplotlib.ticker.MultipleLocator(base=32)\n",
        "ax.xaxis.set_major_locator(loc)\n",
        "ax.yaxis.set_major_locator(loc)\n",
        "ax.grid(which=\"major\", axis=\"both\", linestyle=\"-\", linewidth=3)\n",
        "\n",
        "for bbox in bboxes:\n",
        "    xmin, ymin, xmax, ymax = bbox[:-1]\n",
        "    w = xmax - xmin\n",
        "    h = ymax - ymin\n",
        "    with_mask = bbox[-1]\n",
        "    ax.add_patch(Rectangle((xmin, ymin), w, h, fill=False, color=\"red\"))\n",
        "\n",
        "cx = (bboxes[:, 0] + bboxes[:, 2]) / 2\n",
        "cy = (bboxes[:, 1] + bboxes[:, 3]) / 2\n",
        "\n",
        "ax.scatter(cx, cy, color=\"green\", marker=\"o\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6ddzkGdUp54"
      },
      "source": [
        "## Задача 3.2. 0.5 балла.\n",
        "\n",
        "Выход нашей сетки будет несколько больше, чем Bx16x16x6. Почему?\n",
        "\n",
        "Мы решаем задачу, где классов больше одного. Вспомним прошлое дз: target был одним числом, но выход сетки содержал длинный-длинный вектор, из которого мы получали вероятность принадлежности к тому или иному классу. Здесь то же самое, но как бы в двумерии: у каждой клеточки из этих 16*16 будет свой вектор длины C, который мы будем использовать для определения класса.\n",
        "\n",
        "Реализуйте обратное относительно collate_fn преобразования, чтобы декодировать выход нейронной сети. Применив функцию decode_prediction к выходу collate function вы должны получить изначальный набор прямоугольников с корректными размерами и координатами, а также классами. Применив к выходу нейросети мы тоже должны получить набор прямоугольников и тоже с корректными классами.\n",
        "\n",
        "То есть, нужно проделать операции из collate_fn в обратную сторону, но учесть, что у неройнки выход будет чуть длиннее, и там мы должны брать argmax для определения класса.\n",
        "\n",
        "Hint: в target classes идут в конце. В нейронке они тоже будут в конце, но их будет больше 1. Можно проверять число каналов пришедшего объекта, если оно 6, то перед нами target и надо брать значение, которое записано в клеточке. Иначе (каналов больше 6) перед нами выход нейронки, и надо брать самый вероятный из них."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IyxuODIqzpLf"
      },
      "outputs": [],
      "source": [
        "def decode_prediction(pred, upsample=32, threshold=0.7):\n",
        "    b, c, h, w = pred.shape\n",
        "    img_w, img_h = w * upsample, h * upsample\n",
        "    ### YOUR CODE HERE ###\n",
        "\n",
        "    return"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oB7Jy8hEVlah"
      },
      "source": [
        "## Задача 3.3. 1 балл.\n",
        "Реализуйте модель. Первым делом примените первые 4 блока (до layer4 включительно) ResNet50. Далее добавьте несколько блоков (Conv2D, BatchNorm2D, ReLU). Постепенно уменьшайте количество каналов до 5+C, а размер изображения до 16 на 16. Например, 2048 -> 512 -> 128 -> 32 -> 5+C, где С - количество классов в вашем датасете. Размер ядра при этом 3, паддинг 1. Но вариантов много, попробуйте разные! **Последним слоем обязательно должна быть свертка.** Так как все значения, которые мы предсказываем, находятся в отрезке от 0 до 1 (благодаря нормировке с клеточками), мы после финальной свертки еще применим сигмоиду. Для классов в такой постановке это не навредит.\n",
        "\n",
        "Если будете фантазировать, то для получения правильного размера изображения после сети не стесняйтесь применять слои с фильтрами больше 3."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZWnJu9Yl7m3o"
      },
      "outputs": [],
      "source": [
        "C =  # Количество классов в вашем датасете, хоть руками посчитайте, хоть подтяните из словаря классов\n",
        "\n",
        "\n",
        "class Detector(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        model = torchvision.models.resnet50(weights=ResNet50_Weights.DEFAULT)\n",
        "        ### YOUR CODE HERE ###\n",
        "\n",
        "    def forward(self, img):\n",
        "        ### YOUR CODE HERE ###\n",
        "\n",
        "        return"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-CrPCBlV1Iw"
      },
      "source": [
        "## Задача 3.4. 0.5 балла.\n",
        "\n",
        "Реализуйте функцию потерь.\n",
        "\n",
        "Для этого:\n",
        "* Сделайте маску, которая будет говорить о положении детектируемых объектов. Её нужно использовать с помощью masked_select (см. доки PyTorch)\n",
        "* Лосс похож на оригинальный для Yolo V1 и состоит из 4 частей (reduction='sum' для всех)\n",
        "    - localization loss - Мы берем MSE по координатам бокса там, где есть детектируемый объект\n",
        "    - box_loss - MSE от корней ширины и высоты bbox там, где есть детектируемый объект\n",
        "    - classification_loss - Если детектируемый объект есть, то его кросс-энтропия по его классу\n",
        "    - confidence_loss - Бинарная кросс-энтропия факта наличия объекта ДЛЯ ВСЕХ пикселей. Делается отдельно для детектируемых объектов (вес 1) и для недетектируемых (вес 0.1 например, поскольку их гораздо больше, но можно экспериментировать)\n",
        "\n",
        "\n",
        "* Ниже есть assert. Если вы экспериментируете с лоссом, он не будет проходить, не обращайте на него внимание. Если будете делать описанное выше, то учтите reduction. Бинарная кросс-энтропия вызывается через BCELoss. Параметр C используется для задачи числа классов. assert написан для 3 классов, в задаче с картами их 6. Подумайте как зависит индексация от параметра C и используйте его."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bTeFFazVCKbm"
      },
      "outputs": [],
      "source": [
        "def special_loss(pred, target, check=False, C=C):\n",
        "    ### YOUR CODE HERE ###\n",
        "    localization_loss =\n",
        "    box_loss =\n",
        "    classification_loss =\n",
        "    confidence_loss =\n",
        "\n",
        "    if not check:\n",
        "        return localization_loss + box_loss + classification_loss + confidence_loss\n",
        "\n",
        "    else:\n",
        "        return localization_loss, box_loss, classification_loss, confidence_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f9dbIjtql5Lv"
      },
      "outputs": [],
      "source": [
        "# localization box classification confidence - возвращаются в таком порядке, можно сравнить\n",
        "assert special_loss(torch.zeros((10, 8, 16, 16)), torch.ones((10, 8, 16, 16)), check=True, C=3) == (torch.tensor(5120.), torch.tensor(5120.), torch.tensor(2812.4465), torch.tensor(256000.))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iE5yID_-XUEv"
      },
      "source": [
        "# Задача 4. 2 балла.\n",
        "\n",
        "Обучите вашу модель (написав цикл обучения), и покажите что она работает (скорее всего, объекты найдутся на 1-2 картинках)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "39dMMAu49odu"
      },
      "outputs": [],
      "source": [
        "loader = torch.utils.data.DataLoader(train_ds, 10, collate_fn=collate_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GQiDhPZlN7OR"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "torch.manual_seed(21)\n",
        "EPOCHS =  # Harry Potter 20, Cards 15\n",
        "model = Detector().to(device)\n",
        "opt = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
        "\n",
        "for e in tqdm(range(EPOCHS)):\n",
        "    pbar = tqdm(loader, leave=False)\n",
        "    epoch_losses = []\n",
        "    for batch in pbar:\n",
        "        ### YOUR CODE HERE ###\n",
        "\n",
        "\n",
        "    print(f\"Epoch {e} done; Train loss {np.mean(epoch_losses):.3f};\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ocp2M0-zWiUT"
      },
      "source": [
        "Запустим обученный детектор на тестовых изображениях:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ukgmauCoEWWk"
      },
      "outputs": [],
      "source": [
        "test_loader = torch.utils.data.DataLoader(test_ds, 6, collate_fn=collate_fn)\n",
        "i = iter(test_loader)\n",
        "batch = next(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m7oY4PtfPn1T"
      },
      "outputs": [],
      "source": [
        "# Нужно сделать предсказание и переложить результат на cpu\n",
        "\n",
        "### YOUR CODE HERE ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YBdWfhzt0Slq"
      },
      "outputs": [],
      "source": [
        "# Сделайте визуализацию. Поиграйтесь с threshold, скорее всего нужно понизить до ~0.1\n",
        "\n",
        "### YOUR CODE HERE ###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1TbzYExuuqO1"
      },
      "source": [
        "Результат сильно так себе, да? Есть множество вариантов улучшений, самый простой из которых это приделать к выходу [NMS](https://paperswithcode.com/method/non-maximum-suppression#:~:text=Non%20Maximum%20Suppression%20is%20a,below%20a%20given%20probability%20bound.). Если хочется, можно почитать про YOLO v1 [тут](https://arxiv.org/abs/1506.02640)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sbKKfFh4C9yV"
      },
      "source": [
        "# Задача 5. 3.5 балла.\n",
        "\n",
        "Займёмся более простыми вещами. Возьмем готовую архитектуру, обучим её на наших данных и посмотрим.\n",
        "\n",
        "Для этого будем использовать YOLO v8 от ultralytics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sNXazCJnA0rd"
      },
      "outputs": [],
      "source": [
        "!pip install ultralytics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78dNxuJsA0rd"
      },
      "source": [
        "## Задача 5.1. 1.5 балла.\n",
        "\n",
        "Чтобы дальше модель обучалась одной строкой, данные нужно переложить в правильный формат. Да-да, классика перекладывания JSON. Как правильно паковать можно посмотреть [тут](https://roboflow.com/formats/yolov8-pytorch-txt).\n",
        "\n",
        "Если коротко:\n",
        "* Есть .yaml, где живут пути к папкам с картинками, количество классов и их названия\n",
        "* Есть папочки train valid (их поможем вам собрать), в них две подпапки:\n",
        "    - Первая images, в ней лежат картинки\n",
        "    - Вторая labels, в ней лежат файлы с названиями как у картинок, но вместо расширения картинок нужен .txt, внутри формат как описан на Roboflow\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PHSn1qlyA0rd"
      },
      "outputs": [],
      "source": [
        "# Делаем папочки\n",
        "!rm -rf train\n",
        "!rm -rf valid\n",
        "!mkdir -p train/images train/labels valid/images valid/labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rShRY2NtA0rn"
      },
      "source": [
        "Реализуйте функцию, которая принимает аннотации в изначальном формате, а возвращает их в нужном для YOLO v8. Это должен быть массив готовых строк, которые можно сразу забрасывать в файлик, добавив \\n."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "70rHtPORA0rn"
      },
      "outputs": [],
      "source": [
        "def annotation2txt(bboxes, w_im, h_im):\n",
        "    ### YOUR CODE HERE ###\n",
        "\n",
        "    return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "47EInQx-A0ro"
      },
      "outputs": [],
      "source": [
        "# Копируем картиночки по папочкам и создаем txt файлики\n",
        "\n",
        "for i in range(len(train_ds)):\n",
        "    result = train_ds.__get_raw_item__(i)\n",
        "\n",
        "    shutil.copyfile(\n",
        "        result[0], \"./train/images/\" + result[0].split(\"/\")[-1],\n",
        "    )\n",
        "\n",
        "    h_im, w_im, ch = np.array(Image.open(result[0])).shape\n",
        "    with open(\n",
        "        \"./train/labels/\" + result[0].split(\"/\")[-1].split(\".\")[0] + \".txt\",\n",
        "        \"w\",\n",
        "        encoding=\"utf8\",\n",
        "    ) as f:\n",
        "        f.write(\"\\n\".join(annotation2txt(result[1], w_im, h_im)))\n",
        "\n",
        "for i in range(len(test_ds)):\n",
        "    result = test_ds.__get_raw_item__(i)\n",
        "\n",
        "    shutil.copyfile(\n",
        "        result[0], \"./valid/images/\" + result[0].split(\"/\")[-1],\n",
        "    )\n",
        "\n",
        "    with open(\n",
        "        \"./valid/labels/\" + result[0].split(\"/\")[-1].split(\".\")[0] + \".txt\",\n",
        "        \"w\",\n",
        "        encoding=\"utf8\",\n",
        "    ) as f:\n",
        "        f.write(\"\\n\".join(annotation2txt(result[1], w_im, h_im)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7rziJVW6A0ro"
      },
      "outputs": [],
      "source": [
        "# Собираем YAML\n",
        "\n",
        "nc =  # Укажите число классов. Хоть руками, хоть по-умному посчитайте (см. class_dict)\n",
        "names =  # Укажите имена классов. Хоть руками, хоть по-умному посчитайте, это массив строк (см. class_dict)\n",
        "\n",
        "with open(\"data.yaml\", \"w\") as f:\n",
        "    f.write(f\"train: ../train/images\\nval: ../valid/images\\n\\nnc: {nc}\\nnames: {names}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JG3YkSNkA0ro"
      },
      "source": [
        "## Задание 5.2. 1.5 балла.\n",
        "\n",
        "Обучите модель YOLO v8 самого маленького размера. Библиотека максимально friendly, от вас требуется написать две строчки. Модель нужно взять необученную!\n",
        "\n",
        "Подсказка: подумайте зачем вам data.yaml и что такое yolov8n.yaml (не стесняйтесь гуглить)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Ke0n7wiA0ro"
      },
      "outputs": [],
      "source": [
        "import ultralytics\n",
        "\n",
        "### YOUR CODE HERE ###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4H31vNkA0ro"
      },
      "source": [
        "## Задание 5.3. 0.5 балла.\n",
        "\n",
        "Как-нибудь отрисуйте предсказания на валидационной выборке (хотя бы части из 5-10 картинок).\n",
        "\n",
        "Здесь можно использовать костыли с параметром save=True у predict, потом прочитать их чем-нибудь, отрисовать матплотлибом. Есть варианты и получше. Дефолтный show будет пытаться показывать через opencv imshow, он в коллабе работать не будет."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZjeV-C-LA0rp"
      },
      "outputs": [],
      "source": [
        "### YOUR CODE HERE ###"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "type_ipynb_hw": "task"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}